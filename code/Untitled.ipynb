{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyximport; pyximport.install()\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "import lightgbm as lgb\n",
    "\n",
    "NUM_BRANDS = 4000\n",
    "NUM_CATEGORIES = 1000\n",
    "NAME_MIN_DF = 10\n",
    "MAX_FEATURES_ITEM_DESCRIPTION = 50000\n",
    "\n",
    "\n",
    "def rmsle(y, y0):\n",
    "     assert len(y) == len(y0)\n",
    "     return np.sqrt(np.mean(np.power(np.log1p(y)-np.log1p(y0), 2)))\n",
    "    \n",
    "def split_cat(text):\n",
    "    try: return text.split(\"/\")\n",
    "    except: return (\"No Label\", \"No Label\", \"No Label\")\n",
    "    \n",
    "def handle_missing_inplace(dataset):\n",
    "    dataset['general_cat'].fillna(value='missing', inplace=True)\n",
    "    dataset['subcat_1'].fillna(value='missing', inplace=True)\n",
    "    dataset['subcat_2'].fillna(value='missing', inplace=True)\n",
    "    dataset['brand_name'].fillna(value='missing', inplace=True)\n",
    "    dataset['item_description'].fillna(value='missing', inplace=True)\n",
    "\n",
    "\n",
    "def cutting(dataset):\n",
    "    pop_brand = dataset['brand_name'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_BRANDS]\n",
    "    dataset.loc[~dataset['brand_name'].isin(pop_brand), 'brand_name'] = 'missing'\n",
    "    pop_category1 = dataset['general_cat'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_CATEGORIES]\n",
    "    pop_category2 = dataset['subcat_1'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_CATEGORIES]\n",
    "    pop_category3 = dataset['subcat_2'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_CATEGORIES]\n",
    "    dataset.loc[~dataset['general_cat'].isin(pop_category1), 'general_cat'] = 'missing'\n",
    "    dataset.loc[~dataset['subcat_1'].isin(pop_category2), 'subcat_1'] = 'missing'\n",
    "    dataset.loc[~dataset['subcat_2'].isin(pop_category3), 'subcat_2'] = 'missing'\n",
    "\n",
    "\n",
    "def to_categorical(dataset):\n",
    "    dataset['general_cat'] = dataset['general_cat'].astype('category')\n",
    "    dataset['subcat_1'] = dataset['subcat_1'].astype('category')\n",
    "    dataset['subcat_2'] = dataset['subcat_2'].astype('category')\n",
    "    dataset['item_condition_id'] = dataset['item_condition_id'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.163622140884399] Finished to load data\n",
      "Train shape:  (1482535, 8)\n",
      "Test shape:  (693359, 7)\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "train = pd.read_table('../input/train.tsv', engine='c')\n",
    "test = pd.read_table('../input/test.tsv', engine='c')\n",
    "print('[{}] Finished to load data'.format(time.time() - start_time))\n",
    "print('Train shape: ', train.shape)\n",
    "print('Test shape: ', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38.45512557029724] Split categories completed.\n",
      "[38.94119358062744] Handle missing completed.\n",
      "[48.75910687446594] Cut completed.\n",
      "[49.7005078792572] Convert categorical completed\n",
      "[59.380491495132446] Count vectorize `name` completed.\n",
      "[77.34126806259155] Count vectorize `categories` completed.\n",
      "[244.8103744983673] TFIDF vectorize `item_description` completed.\n",
      "[253.23384475708008] Label binarize `brand_name` completed.\n",
      "[256.34525752067566] Get dummies on `item_condition_id` and `shipping` completed.\n",
      "[269.98689794540405] Create sparse merge completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ugur/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[394.92924094200134] Train ridge completed\n",
      "[394.9740400314331] Predict ridge completed\n",
      "Training until validation scores don't improve for 250 rounds.\n",
      "[1000]\ttraining's rmse: 0.473468\tvalid_1's rmse: 0.482502\n",
      "[2000]\ttraining's rmse: 0.453381\tvalid_1's rmse: 0.468036\n",
      "[3000]\ttraining's rmse: 0.441712\tvalid_1's rmse: 0.461814\n",
      "[4000]\ttraining's rmse: 0.433349\tvalid_1's rmse: 0.458137\n",
      "[5000]\ttraining's rmse: 0.426617\tvalid_1's rmse: 0.455455\n",
      "[6000]\ttraining's rmse: 0.421224\tvalid_1's rmse: 0.453773\n",
      "[7000]\ttraining's rmse: 0.416329\tvalid_1's rmse: 0.452502\n",
      "[8000]\ttraining's rmse: 0.411818\tvalid_1's rmse: 0.451675\n",
      "[1726.6609709262848] Predict lgb 1 completed.\n",
      "Training until validation scores don't improve for 250 rounds.\n",
      "[1000]\ttraining's rmse: 0.467335\tvalid_1's rmse: 0.479306\n",
      "[2000]\ttraining's rmse: 0.447946\tvalid_1's rmse: 0.470878\n",
      "Early stopping, best iteration is:\n",
      "[2657]\ttraining's rmse: 0.439462\tvalid_1's rmse: 0.468564\n",
      "[2236.8852214813232] Predict lgb 2 completed.\n"
     ]
    }
   ],
   "source": [
    "nrow_train = train.shape[0]\n",
    "y = np.log1p(train[\"price\"])\n",
    "merge: pd.DataFrame = pd.concat([train, test])\n",
    "submission: pd.DataFrame = test[['test_id']]\n",
    "\n",
    "del train\n",
    "del test\n",
    "gc.collect()\n",
    "\n",
    "merge['general_cat'], merge['subcat_1'], merge['subcat_2'] = \\\n",
    "zip(*merge['category_name'].apply(lambda x: split_cat(x)))\n",
    "merge.drop('category_name', axis=1, inplace=True)\n",
    "print('[{}] Split categories completed.'.format(time.time() - start_time))\n",
    "\n",
    "handle_missing_inplace(merge)\n",
    "print('[{}] Handle missing completed.'.format(time.time() - start_time))\n",
    "\n",
    "cutting(merge)\n",
    "print('[{}] Cut completed.'.format(time.time() - start_time))\n",
    "\n",
    "to_categorical(merge)\n",
    "print('[{}] Convert categorical completed'.format(time.time() - start_time))\n",
    "\n",
    "cv = CountVectorizer(min_df=NAME_MIN_DF)\n",
    "X_name = cv.fit_transform(merge['name'])\n",
    "print('[{}] Count vectorize `name` completed.'.format(time.time() - start_time))\n",
    "\n",
    "cv = CountVectorizer()\n",
    "X_category1 = cv.fit_transform(merge['general_cat'])\n",
    "X_category2 = cv.fit_transform(merge['subcat_1'])\n",
    "X_category3 = cv.fit_transform(merge['subcat_2'])\n",
    "print('[{}] Count vectorize `categories` completed.'.format(time.time() - start_time))\n",
    "\n",
    "tv = TfidfVectorizer(max_features=MAX_FEATURES_ITEM_DESCRIPTION,\n",
    "                     ngram_range=(1, 3),\n",
    "                     stop_words='english')\n",
    "X_description = tv.fit_transform(merge['item_description'])\n",
    "print('[{}] TFIDF vectorize `item_description` completed.'.format(time.time() - start_time))\n",
    "\n",
    "lb = LabelBinarizer(sparse_output=True)\n",
    "X_brand = lb.fit_transform(merge['brand_name'])\n",
    "print('[{}] Label binarize `brand_name` completed.'.format(time.time() - start_time))\n",
    "\n",
    "X_dummies = csr_matrix(pd.get_dummies(merge[['item_condition_id', 'shipping']],\n",
    "                                      sparse=True).values)\n",
    "print('[{}] Get dummies on `item_condition_id` and `shipping` completed.'.format(time.time() - start_time))\n",
    "\n",
    "sparse_merge = hstack((X_dummies, X_description, X_brand, X_category1, X_category2, X_category3, X_name)).tocsr()\n",
    "print('[{}] Create sparse merge completed'.format(time.time() - start_time))\n",
    "\n",
    "X = sparse_merge[:nrow_train]\n",
    "X_test = sparse_merge[nrow_train:]\n",
    "\n",
    "model = Ridge(alpha=.05, copy_X=True, fit_intercept=True, max_iter=100,\n",
    "  normalize=False, random_state=101, solver='auto', tol=0.001)\n",
    "model.fit(X, y)\n",
    "print('[{}] Train ridge completed'.format(time.time() - start_time))\n",
    "predsR = model.predict(X=X_test)\n",
    "print('[{}] Predict ridge completed'.format(time.time() - start_time))\n",
    "\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size = 0.15, random_state = 144) \n",
    "d_train = lgb.Dataset(train_X, label=train_y, max_bin=8192)\n",
    "d_valid = lgb.Dataset(valid_X, label=valid_y, max_bin=8192)\n",
    "watchlist = [d_train, d_valid]\n",
    "\n",
    "params = {\n",
    "    'learning_rate': 0.5,\n",
    "    'application': 'regression',\n",
    "    'max_depth': 3,\n",
    "    'num_leaves': 60,\n",
    "    'verbosity': -1,\n",
    "    'metric': 'RMSE',\n",
    "    'data_random_seed': 1,\n",
    "    'bagging_fraction': 0.5,\n",
    "    'nthread': 4\n",
    "}\n",
    "\n",
    "params2 = {\n",
    "    'learning_rate': 1,\n",
    "    'application': 'regression',\n",
    "    'max_depth': 3,\n",
    "    'num_leaves': 140,\n",
    "    'verbosity': -1,\n",
    "    'metric': 'RMSE',\n",
    "    'data_random_seed': 2,\n",
    "    'bagging_fraction': 1,\n",
    "    'nthread': 4\n",
    "}\n",
    "\n",
    "model = lgb.train(params, train_set=d_train, num_boost_round=8000, valid_sets=watchlist, \\\n",
    "early_stopping_rounds=250, verbose_eval=1000) \n",
    "predsL = model.predict(X_test)\n",
    "\n",
    "print('[{}] Predict lgb 1 completed.'.format(time.time() - start_time))\n",
    "\n",
    "train_X2, valid_X2, train_y2, valid_y2 = train_test_split(X, y, test_size = 0.1, random_state = 101) \n",
    "d_train2 = lgb.Dataset(train_X2, label=train_y2, max_bin=8192)\n",
    "d_valid2 = lgb.Dataset(valid_X2, label=valid_y2, max_bin=8192)\n",
    "watchlist2 = [d_train2, d_valid2]\n",
    "\n",
    "model = lgb.train(params2, train_set=d_train2, num_boost_round=8000, valid_sets=watchlist2, \\\n",
    "early_stopping_rounds=250, verbose_eval=1000) \n",
    "predsL2 = model.predict(X_test)\n",
    "\n",
    "print('[{}] Predict lgb 2 completed.'.format(time.time() - start_time))\n",
    "\n",
    "preds = predsR*0.35 + predsL*0.35 + predsL2*0.3\n",
    "\n",
    "submission['price'] = np.expm1(preds)\n",
    "submission.to_csv(\"submission_ridge_2xlgbm.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
